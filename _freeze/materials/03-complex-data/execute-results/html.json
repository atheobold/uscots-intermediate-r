{
  "hash": "1c163dc950dd3fb36996b1e6d024cfa1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"More Complex Data\"\nsubtitle: \"Beyond CSV Files\"\nformat: \n  revealjs:\n    footer: \"[USCOTS Intermediate R Workshop](https://atheobold.github.io/uscots-intermediate-r/)\"\n    theme: [simple, styles.scss]\n    embed-resources: true\n    scrollable: true\neditor: source\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Why Move Beyond CSV?\n\n. . .\n\nReal-world data comes in many forms:\n\n::: {.incremental}\n- **Web data**: HTML tables, JSON APIs, XML feeds\n- **Real-time data**: APIs that update continuously\n- **Large datasets**: Files too big for memory\n- **Structured databases**: Relational data with foreign keys\n:::\n\n. . .\n\n::: {style=\"font-size: 1.2em; color: #0F4C81;\"}\nToday: Tools and techniques for handling complex data sources\n:::\n\n## Why Integrate Other Data Sources?\n\n::: {.incremental}\n1. Real-world data are rarely nicely formatted\n2. Knowing how to access and format data is crucial\n3. Much of these data are large and (continuously) updating\n4. Simple data sets like `gapminder` and `palmerpenguins` are sometimes not adequate for students to gain practical experience\n:::\n\n\n\n# Web Scraping {background-color=\"#0F4C81\"}\n\n## Web Scraping Fundamentals\n\nWeb scraping extracts data from websites programmatically\n\n::: {.incremental}\n- **`rvest`**: Core web scraping package for R\n- **`polite`**: Ethical scraping practices\n- **`httr2`**: HTTP requests and authentication (we'll get into this with APIs)\n:::\n\n. . .\n\n::: {style=\"font-size: 0.9em; color: #D32F2F;\"}\n**Always check robots.txt and respect rate limits!**\n\n**Web Scraping Ethics:** Use [robots.txt checker](https://www.robotstxt.org/robotstxt.html) to review the \"language\" of this file\n\n**Example:** [Google's robot.txt](https://www.google.com/robots.txt)\n:::\n\n\n## Essential Web Scraping Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pak::pak(c(\"rvest\", \"polite\"))\nlibrary(rvest)\nlibrary(polite)\n```\n:::\n\n\n\n\n. . .\n\n::: {.midi}\n**polite** ensures you:\n\n- Check robots.txt permissions\n- Respect rate limits automatically  \n- Cache requests to avoid redundancy\n:::\n\n## Polite Web Scraping Workflow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Bow and introduce yourself\nsession <- bow(\n  \"https://example.com\", \n  user_agent = \"Your Name <email@example.com>\"\n)\n\n# 2. Scrape politely\nscraped_data <- scrape(session) %>%\n  html_elements(\"title\") %>%\n  html_text()\n```\n:::\n\n\n\n\nWhat would you expect `scraped_data` to contain?\n\n\n\n## Polite Web Scraping Workflow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscraped_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Example Domain\"\n```\n\n\n:::\n:::\n\n\n\n\n## Polite Web Scraping Workflow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Bow and introduce yourself\nsession <- bow(\n  \"https://example.com\", \n  user_agent = \"Your Name <email@example.com>\"\n)\n\n# 2. Scrape politely\nscraped_data <- scrape(session) %>%\n  html_elements(\"title\") %>%\n  html_text()\n```\n:::\n\n\n\n\n::: {.midi}\nThis workflow automatically: 1) Checks robots.txt, 2) Introduces your scraper and 3) Respects crawl delays\n:::\n\n## Web Scraping Resources for Teaching\n\n::: {.small}\n**Essential Learning Materials:**\n\n- [R for Data Science (2e) - Web Scraping Chapter](https://r4ds.hadley.nz/webscraping.html) - Comprehensive beginner tutorial\n- [rvest Package Documentation](https://rvest.tidyverse.org/) - Official `rvest` reference with examples\n- [Web Scraping the Polite Way](https://www.rostrum.blog/posts/2019-03-04-polite-webscrape/) - Ethics-focused tutorial\n- [ZenRows Complete Guide 2025](https://www.zenrows.com/blog/web-scraping-r) - Step-by-step practical guide\n- [Crime by Numbers Chapter 19](https://crimebythenumbers.com/webscraping-with-rvest.html) - Real-world examples\n- [Automated Web Scraping Workshop](https://resulumit.com/teaching/scrp_workshop.html) - Complete lesson plan (187 slides)\n:::\n\n\n\n# APIs & JSON/XML {background-color=\"#0F4C81\"}\n\n## APIs: A Better Way to Get Data\n\nAPIs (Application Programming Interfaces) provide:\n\n::: {.incremental}\n- **Structured access** to data\n- **Rate limiting** built-in\n- **Authentication** systems\n- **Documentation** and support\n:::\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n**NOTE: Always prefer APIs over web scraping when available! But why??**\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\n**Student Exercise ðŸ’¡** Compare same data via API vs scraping ([OpenWeather API](https://openweathermap.org/api) vs weather websites)\n:::\n:::\n\n## Modern R API Tools\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install if needed\n# pak::pak(c(\"httr2\", \"jsonlite\", \"xml2\"))\n\nlibrary(httr2)    # Modern HTTP client\nlibrary(jsonlite) # JSON parsing\nlibrary(xml2)     # XML handling\n```\n:::\n\n\n\n\n. . .\n\n::::: {.midi}\n**`httr2`** features:\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Pipe-friendly syntax\n- Built-in retry logic\n:::\n\n::: {.column width=\"50%\"}\n- Automatic request throttling\n- Better error handling than httr\n:::\n::::\n:::::\n\n## `httr2` API Workflow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create and execute request \nresponse <- request(\"https://api.nasa.gov/insight_weather/\") %>%\n  req_url_query(api_key = \"DEMO_KEY\", feedtype = \"json\", ver = \"1.0\") %>%\n  req_perform()\n\n# Parse JSON response\ndata <- response %>%\n  resp_body_json() \n\ndata %>%\n  purrr::pluck(\"675\", \"AT\") %>% \n  as_tibble() %>% \n  mutate(day = 675)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 5\n     av     ct    mn    mx   day\n  <dbl>  <int> <dbl> <dbl> <dbl>\n1 -62.3 177556 -96.9 -15.9   675\n```\n\n\n:::\n:::\n\n\n\n\n:::{.small}\n**Student Exercise ðŸ’¡** NASA has a lot of APIs to play with!\n:::\n\n## Working with JSON Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert JSON to data frame\njson_data <- '{\n  \"users\": [\n    {\"name\": \"Alice\", \"age\": 25},\n    {\"name\": \"Bob\", \"age\": 30}\n  ]\n}'\n\n# Parse with jsonlite\ndf <- jsonlite::fromJSON(json_data)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$users\n   name age\n1 Alice  25\n2   Bob  30\n```\n\n\n:::\n\n```{.r .cell-code}\ndf[[\"users\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   name age\n1 Alice  25\n2   Bob  30\n```\n\n\n:::\n:::\n\n\n\n\n## API & JSON/XML Teaching Resources\n\n::: {.small}\n**Core Documentation & Tutorials:**\n\n- [httr2 Package Documentation](https://httr2.r-lib.org/) - Official reference\n- [Wrapping APIs with httr2](https://httr2.r-lib.org/articles/wrapping-apis.html) - Advanced techniques\n- [jsonlite: JSON APIs Vignette](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-apis.html) - JSON parsing examples\n- [Working with Web Data APIs (R-bloggers)](https://www.r-bloggers.com/2021/07/working-with-web-data-in-r-part-ii-apis/) - Conceptual overview\n\n**Student-Friendly Tutorials:**\n\n- [DataScience+ API Tutorial](https://datascienceplus.com/accessing-web-data-json-in-r-using-httr/) - Step-by-step guide\n- [RPubs REST API Tutorial](https://rpubs.com/ankc/480665) - Practical examples\n- [MockUp Blog JSON Parsing](https://themockup.blog/posts/2020-05-22-parsing-json-in-r-with-jsonlite/) - Real-world data\n:::\n\n\n\n# Large Data Performance {background-color=\"#0F4C81\"}\n\n## When Data Gets Big\n\nTraditional R tools struggle with:\n\n::: {.incremental}\n- **Files larger than RAM**\n- **Millions of rows** \n- **Complex joins** across tables\n- **Repeated analysis** on same data\n:::\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #0F4C81;\"}\n**Solution: Modern high-performance tools**\n:::\n\n## The Performance Trio\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install if needed\n# pak::pak(c(\"data.table\", \"arrow\", \"duckdb\"))\n\n# High-performance data processing\nlibrary(data.table)  # In-memory speed\nlibrary(arrow)       # Columnar storage and data transfer\nlibrary(duckdb)      # In-process analytics\n```\n:::\n\n\n\n\n. . .\n\n::: {.midi}\n**Performance gains:**\n\n- **`data.table`**: 10-100x faster than base R\n- **`arrow` + `duckdb`**: Handle larger-than-memory data\n- **Combined**: Up to 20x faster pipelines\n:::\n\n## `data.table`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read large parquet file\nlarge_data <- data.table::fread(here::here(\"materials\", \"data\", \"large_data.csv\"))\n\n# data.table\nsetDT(large_data)\nlarge_data[year == 2023, .(avg_value = mean(value)), by = species]\n```\n:::\n\n\n\n\n::: {.midi}\nBased on this code alone, can you guess what is happening to `large_data`?\n:::\n\n## `data.table`\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\nDepending on student needs, `data.table` can be a very useful R development package:\n\n* careful dev cycle\n* concise syntax\n* efficient and fast in memory data\n\nThousands of R packages use on `data.table` for these reasons!\n:::\n\n\n## Arrow + DuckDB Integration\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read large parquet file\nhuge_data <- arrow::open_dataset(here::here(\"materials\", \"data\", \"huge_data.parquet\"))\n\n# Query with DuckDB (SQL or dplyr syntax)\nresult <- huge_data %>%\n  arrow::to_duckdb() %>%\n  filter(year == 2023) %>%\n  group_by(species) %>%\n  summarise(avg_value = mean(bill_length_mm)) %>%\n  collect()\n```\n:::\n\n\n\n\n::: {.midi}\n**Zero-copy integration** between Arrow and DuckDB!\n:::\n\n## Why This Approach Works\n\n::: {.incremental}\n- **Arrow**: Columnar format, efficient I/O\n- **DuckDB**: In-process OLAP database\n- **Zero-copy**: No serialization overhead\n- **Familiar syntax**: Works with `dplyr`\n:::\n\n. . .\n\n::: {style=\"font-size: 1.1em; color: #2E7D32;\"}\n**This means that 33.8 GB data analyzed in 16 GB RAM**\n:::\n\n## Large Data Teaching Resources\n\n::: {.small}\n**Technical Documentation:**\n\n- [The Raft](https://rdatatable-community.github.io/The-Raft/) - `data.table` community blog\n- [Arrow + DuckDB Integration Guide](https://duckdb.org/2021/12/03/duck-arrow.html) - Zero-copy concepts\n- [Arrow R Package Documentation](https://arrow.apache.org/docs/r/) - Complete reference\n- [DuckDB R Documentation](https://duckdb.org/docs/api/r) - SQL in R examples\n\n**Teaching-Focused Materials:**\n\n- [NCEAS Arctic Research Tutorial](https://learning.nceas.ucsb.edu/2025-04-arctic/sections/parquet-arrow.html) - Academic workshop\n- [Christophe Nicault's Tutorial](https://www.christophenicault.com/post/large_dataframe_arrow_duckdb/) - Step-by-step guide\n- [Thomas Mock's Bigger Data Tutorial](https://jthomasmock.github.io/bigger-data/) - Practical examples\n\n**Performance Comparisons for Context:**\n\n- [Appsilon Performance Study](https://www.appsilon.com/post/r-data-processing-frameworks) - Benchmarking results\n- [R-bloggers Larger-than-Memory Guide](https://www.r-bloggers.com/2022/11/handling-larger-than-memory-data-with-arrow-and-duckdb/) - Practical approach\n:::\n\n# Databases {background-color=\"#0F4C81\"}\n\n## Why Use Databases?\n\nDatabases provide:\n\n::: {.incremental}\n- **Structured storage** with relationships\n- **ACID compliance** for data integrity\n- **Concurrent access** for teams\n- **Scalability** beyond single machines\n- **SQL ecosystem** and tools\n:::\n\n## R Database Ecosystem\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install if needed\n# pak::pak(c(\"DBI\", \"RSQLite\", \"RPostgreSQL\", \"RMySQL\", \"dbplyr\"))\n\nlibrary(DBI)         # core database interface\nlibrary(RSQLite)     # SQLite (embedded)\nlibrary(RPostgreSQL) # PostgreSQL\nlibrary(RMySQL)      # MySQL/MariaDB\nlibrary(dbplyr)      # Database-aware dplyr\n```\n:::\n\n\n\n\n## Basic Database Workflow\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Connect to database\ncon <- dbConnect(RSQLite::SQLite(), \"mydata.db\")\n\n# Write data to database\ndbWriteTable(con, \"customers\", customer_data)\n\n# Query with SQL\nresult <- dbGetQuery(con, \"\n  SELECT customer_id, SUM(amount) as total\n  FROM orders \n  GROUP BY customer_id\n\")\n\n# Always disconnect\ndbDisconnect(con)\n```\n:::\n\n\n\n\n## Database with `dplyr` Syntax\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Connect to table as dplyr object\ncustomers_tbl <- tbl(con, \"customers\")\n\n# Use familiar dplyr syntax\nsummary <- customers_tbl %>%\n  filter(status == \"active\") %>%\n  group_by(region) %>%\n  summarise(avg_value = mean(lifetime_value)) %>%\n  collect()  # Execute and bring to R\n```\n:::\n\n\n\n\n::: {.midi}\n**`dbplyr` translates dplyr code to SQL automatically!**\n\n::: {style=\"font-size: 0.9em; color: #666;\"}\n**Teaching Moment:** Use `show_query()` to reveal the generated SQL - great for connecting R to SQL concepts\n:::\n:::\n\n## Database Teaching Resources\n\n::: {.small}\n**Core Textbook Materials:**\n\n- [R for Data Science (2e) - Databases Chapter](https://r4ds.hadley.nz/databases.html) - Student-friendly introduction\n- [DBI Package Documentation](https://dbi.r-dbi.org/) - Complete reference guide\n- [RSQLite Tutorial & Vignette](https://rsqlite.r-dbi.org/) - Embedded database examples\n\n**Teaching-Ready Tutorials:**\n\n- [R-Coder SQL Tutorial](https://r-coder.com/sql-r/) - Step-by-step with code\n- [SQL Docs R Tutorial](https://sqldocs.org/sqlite-database/sqlite-in-r/) - Interactive workflows\n- [AWS R and Databases Tutorial](https://rstudio-pubs-static.s3.amazonaws.com/52614_1fa12c657ba7492092bd538205d7f02e.html) - Cloud perspective\n\n**Official Package Documentation:**\n\n- [dbplyr Documentation](https://dbplyr.tidyverse.org/) - dplyr translation to SQL\n- [Databases using R (RStudio)](https://edgararuiz.github.io/db.rstudio.com/databases/sqlite.html) - Best practices\n:::\n\n## Summary: Your Complex Data Toolkit\n\n::: {.midi}\n| **Data Source** | **Primary Tools** | **Best For** |\n|:----------------|:------------------|:-------------|\n| **Web Pages** | rvest, polite | Structured web data |\n| **APIs** | httr2, jsonlite | Real-time, authenticated data |\n| **Large Files** | arrow, duckdb | Bigger-than-memory analysis |\n| **Databases** | DBI, dbplyr | Structured, relational data |\n:::\n\n. . .\n\n::: {style=\"font-size: 1.2em; color: #0F4C81;\"}\n**Choose the right tool for your data source and size!**\n:::\n\n\n## Ideas for Projects\n\n::: {.midi}\n**BRAINSTORM:** What kinds of projects could you include that:\n\n1. Gives students exposure to these data sources\n2. While continuing to focus on the topic of your course\n:::\n\n\n\n## Additional Teaching Resources\n\n::: {.small}\n**Course Syllabi & Examples:**\n\n- [UC Davis STA 141C](https://github.com/nick-ulle/2022-sta141c/) - Complete web scraping course\n- [Data Science in a Box](https://datasciencebox.org/) - Includes web data modules\n\n**Practice Datasets & APIs:**\n\n- [JSONPlaceholder](https://jsonplaceholder.typicode.com/) - Fake REST API for testing\n- [httpbin.org](https://httpbin.org/) - HTTP request/response service\n- [OpenWeather API](https://openweathermap.org/api) - Real weather data with free tier\n- [REST Countries API](https://restcountries.com/) - Country data, no auth required\n\n**Ethics & Legal Resources:**\n\n- [robots.txt Specification](https://www.robotstxt.org/) - Official standard\n- [Web Scraping Ethics Guide](https://blog.apify.com/is-web-scraping-legal/) - Legal considerations\n- [API Terms of Service Examples](https://tosdr.org/) - Teaching about data use agreements\n:::\n\n\n## Create your Project A Data Analysis assignment\n\nThings to include:\n\n* Will you require any **specific research questions** to be addressed, or is it open-ended?\n\n* Will you require any **specific elements**, such as \"join at least two datasets\" or \"use a dataset of at least 10000 rows\"?\n\n* How **technical** should the report be - is the audience other data scientists, or general public?\n\n* How will you grade **successful but inefficient pipelines**?\n\n* How will you make sure the pipeline **really accomplishes** what the report claims?\n",
    "supporting": [
      "03-complex-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}